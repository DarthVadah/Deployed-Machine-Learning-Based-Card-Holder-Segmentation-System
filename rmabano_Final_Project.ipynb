{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4137cc6",
   "metadata": {},
   "source": [
    "## Python For Data Analytics\n",
    "### Final Project\n",
    "#### Notebook by: rmabano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cc6619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Basic essential Libraries\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd981e0",
   "metadata": {},
   "source": [
    "### 1. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085c014b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "Credit_Card_data = pd.read_csv('CC GENERAL.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3eacd2",
   "metadata": {},
   "source": [
    "### 2. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce045b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows of the dataset\n",
    "Credit_Card_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d78811f",
   "metadata": {},
   "outputs": [],
   "source": [
    "CC_INFO = Credit_Card_data.info()\n",
    "print(CC_INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cf53c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the mathematical statistics/characteristics \n",
    "Credit_Card_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3a4413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the distribution of each feature or column\n",
    "plt.figure(figsize=(15, 12))\n",
    "for i, col in enumerate(Credit_Card_data.columns[1:]):  # Excluding CUST_ID for visualization\n",
    "    plt.subplot(5, 4, i+1)\n",
    "    sns.histplot(Credit_Card_data[col], kde=True)\n",
    "    plt.title(col)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf41da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for missing values\n",
    "Credit_Card_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bade147",
   "metadata": {},
   "outputs": [],
   "source": [
    "#USING BOXPLOTS TO CHECK FOR OUTLIERS\n",
    "# Plotting box plots for each numerical feature\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, col in enumerate(Credit_Card_data.columns[1:]):  # Excluding CUST_ID for visualization\n",
    "    plt.subplot(5, 4, i+1)\n",
    "    sns.boxplot(y=Credit_Card_data[col])\n",
    "    plt.title(col)\n",
    "    plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9867227",
   "metadata": {},
   "source": [
    "### 3. Data Preprocessing (Handling Missing Values, Outliers, and Encoding) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8b0472",
   "metadata": {},
   "source": [
    "#### Handling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233eef99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling missing values by replacing them with the mean of each column\n",
    "Credit_Card_data.fillna(Credit_Card_data.mean(), inplace=True)\n",
    "\n",
    "# Recheck missing values to ensure they are filled\n",
    "Credit_Card_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51703816",
   "metadata": {},
   "source": [
    "#### Handling Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c703bd98",
   "metadata": {},
   "source": [
    "Dealing with outliers is a crucial step in preparing the data for analysis and modeling. There are several strategies for handling outliers. Given the nature of this dataset (financial transactions), it might be more appropriate to use methods that retain the outliers but reduce their impact, rather than simply removing them. Two common approaches are:\n",
    "\n",
    "* Log Transformation: This is effective for right-skewed distributions. It can't be applied directly to values of zero or negative values, so we need to adjust for that.\n",
    "* Capping: Outliers are capped at a certain percentile. For instance, values above the 95th percentile can be set to the 95th percentile value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da595f8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This code will apply a log transformation to each numerical feature, adjusting for zero values. \n",
    "After the transformation, we'll visualize the distributions again to see the effect.\n",
    "Applying log transformation to features with significant outliers.\n",
    "\n",
    "\"\"\"\n",
    "# Adding 1 to avoid log(0) which is undefined\n",
    "for col in Credit_Card_data.columns[1:]:  # Excluding CUST_ID for transformation\n",
    "    if Credit_Card_data[col].min() > 0:  # If no zero or negative values in the column\n",
    "        Credit_Card_data[col] = np.log(Credit_Card_data[col])\n",
    "    else:\n",
    "        Credit_Card_data[col] = np.log(Credit_Card_data[col] + 1)  # Adjusting for zero values\n",
    "\n",
    "# Visualizing the distributions post-transformation\n",
    "plt.figure(figsize=(15, 12))\n",
    "for i, col in enumerate(Credit_Card_data.columns[1:]):  # Excluding CUST_ID for visualization\n",
    "    plt.subplot(5, 4, i+1)\n",
    "    sns.histplot(Credit_Card_data[col], kde=True)\n",
    "    plt.title(col)\n",
    "    plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8ae63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#USING BOXPLOTS TO CHECK FOR OUTLIERS\n",
    "# Plotting box plots for each numerical feature\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, col in enumerate(Credit_Card_data.columns[1:]):  # Excluding CUST_ID for visualization\n",
    "    plt.subplot(5, 4, i+1)\n",
    "    sns.boxplot(y=Credit_Card_data[col])\n",
    "    plt.title(col)\n",
    "    plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afd4fae",
   "metadata": {},
   "source": [
    "**Post-Log Transformation Distributions**\n",
    "\n",
    "After applying the log transformation to the dataset (excluding CUST_ID), we have the following observations:\n",
    "\n",
    "* Improved Distributions: The log transformation has significantly improved the skewness in most features. The distributions now appear more normalized, which is beneficial for many statistical models and machine learning algorithms.\n",
    "\n",
    "* Reduced Impact of Outliers: The transformation has reduced the extreme values' impact, making the dataset more uniform and less skewed.\n",
    "\n",
    "**Implications for Modeling**\n",
    "\n",
    "* For this dataset, log transformation was a suitable initial approach due to the nature of the data and the goal of retaining as much information as possible. Whether to also apply capping depends on further analysis and the specific requirements of the subsequent modeling steps.\n",
    "\n",
    "* Enhanced Model Performance: Many algorithms perform better when the data does not have extreme values or heavy skewness. The log transformation thus potentially enhances model performance.\n",
    "\n",
    "* Feature Interpretation: Post-transformation, the features now represent the logarithm of their original values. This should be kept in mind while interpreting the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba14005",
   "metadata": {},
   "source": [
    "#### Label Encoding (If needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d6a7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for categorical variables so that they are encoded\n",
    "categorical_cols = Credit_Card_data.select_dtypes(include=['object']).columns\n",
    "categorical_cols\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ab142c",
   "metadata": {},
   "source": [
    "The dataset contains only one categorical column, CUST_ID, which is an identifier and not a feature for modeling. Hence, no label encoding is required for the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25e8ab8",
   "metadata": {},
   "source": [
    "#### Data Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104c4773",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# Scaling the data excluding CUST_ID\n",
    "scaler = StandardScaler()\n",
    "CC_data_scaled = Credit_Card_data.copy()\n",
    "\n",
    "#SCALING COLUMNS EXCEPT FOR THE CUST_ID DATA\n",
    "CC_data_scaled[Credit_Card_data.columns[1:]] = scaler.fit_transform(Credit_Card_data[Credit_Card_data.columns[1:]])\n",
    "\n",
    "# Saving the scaler for later use\n",
    "with open('standard_scaler.pkl', 'wb') as file:\n",
    "    pickle.dump(scaler, file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e4d1a7",
   "metadata": {},
   "source": [
    "**Data Scaling**\n",
    "\n",
    "- The numerical features (excluding CUST_ID) have been scaled using the Standard Scaler.\n",
    "- This scaling is essential for algorithms that are sensitive to the scale of the data.\n",
    "\n",
    "**Standard Scaler Serialization**\n",
    "\n",
    "- The fitted Standard Scaler has been saved as a pickle file standard_scaler.pkl.\n",
    "\n",
    "- This scaler can be retrieved during model deployment to ensure that new data is scaled consistently with the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d93d366",
   "metadata": {},
   "source": [
    "### 4. Unsupervised Model Creation & Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0577ad34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "#finfing optimum k using \n",
    "\n",
    "# Elbow Method\n",
    "inertia = []\n",
    "K = range(1, 11)  # Testing 1 to 10 clusters\n",
    "for k in K:\n",
    "    kmeanModel = KMeans(n_clusters=k)\n",
    "    kmeanModel.fit(CC_data_scaled.drop('CUST_ID', axis=1))\n",
    "    inertia.append(kmeanModel.inertia_)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(K, inertia, 'bx-')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('The Elbow Method showing the optimal k')\n",
    "\n",
    "# Identifying the \"elbow\" point and annotating it on the plot\n",
    "plt.annotate('Optimal K', xy=(2, inertia[1]), xytext=(3, inertia[1] + 2000),\n",
    "             arrowprops=dict(facecolor='black', arrowstyle='->'),)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650e30f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "def calculate_silhouette_scores(data):\n",
    "    \"\"\"\n",
    "    This function will run the silhouette Score test for each cluster from 2-11.\n",
    "    This metric is used to validate the choice we made in the cell above.\n",
    "    \n",
    "    \"\"\"\n",
    "    silhouette_scores = []\n",
    "    \n",
    "    for k in range(2, 12):  # Testing clusters from 2 to 11\n",
    "        kmeans = KMeans(n_clusters=k)\n",
    "        cluster_labels = kmeans.fit_predict(data.drop('CUST_ID', axis=1))\n",
    "        silhouette_avg = silhouette_score(data.drop('CUST_ID', axis=1), cluster_labels)\n",
    "        silhouette_scores.append(silhouette_avg)\n",
    "        print(f\"For k = {k}, Silhouette Score = {silhouette_avg}\")\n",
    "\n",
    "    return silhouette_scores\n",
    "\n",
    "silhouette_scores = calculate_silhouette_scores(CC_data_scaled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3842ec69",
   "metadata": {},
   "source": [
    "As can be seen from the chosen K values, 2 provides the highest Silhouette Score, making it the best cluster value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188c802a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proceeding with KMeans clustering\n",
    "kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "CC_data_scaled['Cluster'] = kmeans.fit_predict(CC_data_scaled.drop('CUST_ID', axis=1))\n",
    "\n",
    "# Saving the labeled dataset\n",
    "output_file = 'rmabano-cc-labelled.csv'\n",
    "CC_data_scaled.to_csv(output_file, index=False)\n",
    "\n",
    "print('You have successfully generated:', output_file)  # Returning the count of NaN values after imputation and the file path for download\n",
    "\n",
    "\n",
    "# labelled_dataset.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70cf3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Now we visualize the clusters we have created with the help of PCA\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Loading the labeled dataset\n",
    "labelled_dataset = pd.read_csv('rmabano-cc-labelled.csv')\n",
    "\n",
    "labelled_dataset.fillna(Credit_Card_data.mean(), inplace=True)\n",
    "\n",
    "# Applying PCA to reduce the data to two dimensions for visualization\n",
    "pca = PCA(n_components=2)\n",
    "labeled_data_pca = pca.fit_transform(labelled_dataset.drop(['CUST_ID', 'Cluster'], axis=1))\n",
    "\n",
    "# Creating a DataFrame for the PCA results\n",
    "pca_df = pd.DataFrame(data=labeled_data_pca, columns=['PCA1', 'PCA2'])\n",
    "pca_df['Cluster'] = labelled_dataset['Cluster']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Adjusting the scatter plot as per the new requirements\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Cluster 0: Marked with 'x' and in red color\n",
    "plt.scatter(pca_df[pca_df['Cluster'] == 0]['PCA1'], pca_df[pca_df['Cluster'] == 0]['PCA2'], \n",
    "            label='Cluster 0', alpha=0.5, marker='x', color='red')\n",
    "\n",
    "# Cluster 1: Marked with circles and in blue color\n",
    "plt.scatter(pca_df[pca_df['Cluster'] == 1]['PCA1'], pca_df[pca_df['Cluster'] == 1]['PCA2'], \n",
    "            label='Cluster 1', alpha=0.5, marker='o', color='blue')\n",
    "\n",
    "plt.title('2D PCA of Credit Card Dataset')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01ca5c7",
   "metadata": {},
   "source": [
    "The 2D PCA scatter plot above visualizes the two clusters (Cluster 0 and Cluster 1) in the credit card dataset. Each point represents a customer, and the color indicates the cluster to which the customer belongs. \n",
    "This visualization helps to see how well-separated the clusters are and provides a visual representation of the customer segmentation. Each point represents a customer, and the markers and colors help to visually differentiate between the customer segments based on their clustering:\n",
    "\n",
    "- Cluster 0 is represented by red 'x' markers.\n",
    "- Cluster 1 is represented by blue circle markers.\n",
    "\n",
    "Next, let's analyze the cluster centroids to understand the characteristics of each cluster. This involves examining the average values of the original features for each cluster, which can provide insights into what defines each segment. Let's proceed with this analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24afc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "evaluating the kmeans model\n",
    "\"\"\"\n",
    "kmeans = KMeans(n_clusters=2, n_init=10, random_state=42, max_iter=500)\n",
    "\n",
    "clust_lab = kmeans.fit_predict(CC_data_scaled.drop(columns = \"CUST_ID\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c87350d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "EVALUATION METRIC 1: Silhouette Score\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Print the silhouette score\n",
    "k_sil_score = silhouette_score(pca_df, kmeans.labels_)\n",
    "print(f\"Silhouette score : {k_sil_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1897f383",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score\n",
    "\n",
    "\"\"\"\n",
    "EVALUATION METRIC 2: Calinski Harabasz Score\n",
    "\"\"\"\n",
    "\n",
    "Harabasz_score = calinski_harabasz_score(pca_df, kmeans.labels_)\n",
    "print(f\"Calinski and Harabasz score : {Harabasz_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1036e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the centroids of each cluster\n",
    "cluster_centroids = labelled_dataset.groupby('Cluster').mean()\n",
    "\n",
    "# Displaying the centroids\n",
    "cluster_centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13902bf2",
   "metadata": {},
   "source": [
    "The table above shows the centroids of each cluster, representing the average values of each feature within the cluster. These centroids help us understand the defining characteristics of each customer segment:\n",
    "\n",
    "**Cluster 0:**\n",
    "\n",
    "- This cluster tends to have lower balances and less frequent balance updates.\n",
    "- Customers in this cluster make more purchases, both one-off and installment, compared to Cluster 1.\n",
    "- They use cash advances less frequently and have fewer cash advance transactions.\n",
    "- They have a higher frequency of purchases and are more likely to make purchases in installments.\n",
    "- These customers generally have a higher full payment rate and slightly longer tenure.\n",
    "\n",
    "\n",
    "**Cluster 1:**\n",
    "\n",
    "- Customers in this cluster have higher balances and more frequent balance updates.\n",
    "- They make fewer purchases, both one-off and installment.\n",
    "- This cluster uses cash advances more frequently and has more cash advance transactions.\n",
    "- They have a lower frequency of purchases and are less likely to make purchases in installments.\n",
    "- These customers have a lower full payment rate and slightly shorter tenure.\n",
    "\n",
    "These insights can be used to tailor marketing strategies, product offerings, and services to each customer segment. For example, customers in Cluster 0 might be more responsive to promotions related to installment purchases, while those in Cluster 1 might be targeted with offers related to cash advances or products for customers with higher balances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509018e6",
   "metadata": {},
   "source": [
    "### 5. Supervised Model Creation & Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50b4447",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "\"\"\"\n",
    "Building model with all 17 features\n",
    "\"\"\"\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Separating the features and the target variable\n",
    "X = labelled_dataset.drop(['CUST_ID', 'Cluster'], axis=1)\n",
    "y = labelled_dataset['Cluster']\n",
    "\n",
    "# Splitting the dataset into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize Random Forest Classifier\n",
    "First_rf_model = RandomForestClassifier(max_depth=5, random_state=42)\n",
    "\n",
    "# Perform cross-validation\n",
    "init_cv_scores = cross_val_score(First_rf_model, X_train, y_train, cv=10)\n",
    "\n",
    "# Train the model on the entire training set\n",
    "First_rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = First_rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Calculate additional metrics\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "auc_roc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "# Store all metrics in a dictionary\n",
    "initial_model_metrics = {\n",
    "    'Accuracy': accuracy,\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F1 Score': f1,\n",
    "    'AUC-ROC': auc_roc\n",
    "}\n",
    "\n",
    "## Saving the model\n",
    "model_save_path = 'random_forest_classifier.pkl'\n",
    "with open(model_save_path, 'wb') as file:\n",
    "    pickle.dump(First_rf_model, file)\n",
    "    \n",
    "print(\"Model Performance Metrics:\")\n",
    "for metric, value in initial_model_metrics.items():\n",
    "    print(f\"{metric}: {value}\")\n",
    "\n",
    "print(f\"Cross-validation scores: {init_cv_scores}\")\n",
    "print(f\"Accuracy on test set: {accuracy}\")\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205b49d3",
   "metadata": {},
   "source": [
    "Learning Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a372253e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "# Function to plot the learning curves\n",
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None, n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    \n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "    \n",
    "    # Mean and Standard Deviation of training scores\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    \n",
    "    # Mean and Standard Deviation of cross-validation scores\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt\n",
    "\n",
    "# Plotting the learning curve for the Random Forest Classifier\n",
    "plot_learning_curve(First_rf_model, \"Learning Curves (Random Forest)\", X_train, y_train, cv=5, n_jobs=4)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0562f71",
   "metadata": {},
   "source": [
    "**Insight From Curve:**\n",
    "\n",
    "1. **High Training Score:**\n",
    "   - The training score starts and remains high as more data points are added. This indicates that the model is able to fit the training data very well.\n",
    "\n",
    "2. **Converging Scores:**\n",
    "   - The cross-validation score increases with the number of training examples and is converging towards the training score. This suggests that the model is generalizing well and is not overfitting. The gap between the training and cross-validation scores is small, which is a good sign.\n",
    "\n",
    "3. **Plateauing of Scores:**\n",
    "   - Both scores plateau towards the right end of the graph, which suggests that adding more training data might not lead to significant improvements in the model's performance. The model seems to have learned as much as it can about the data and reached its performance limit given the current feature set and model complexity.\n",
    "\n",
    "4. **No High Bias or High Variance:**\n",
    "   - There's no evidence of high bias (underfitting) as both the training and validation scores are high.\n",
    "   - There's no evidence of high variance (overfitting) as the training and validation scores are close to each other.\n",
    "\n",
    "In conclusion, the learning curves indicate that the Random Forest model is performing well on this dataset. There doesn't appear to be a problem with overfitting or underfitting. The model seems to have reached a good balance between bias and variance, providing good generalization performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df50f09b",
   "metadata": {},
   "source": [
    "#### Reasons for choosing Random Forest Classifier Model\n",
    "\n",
    "When selecting a machine learning algorithm for a particular task, several factors are considered, including the nature of the data, the complexity of the problem, the interpretability of the model, and computational efficiency. In the case of our credit card dataset, **I opted for the Random Forest Classifier** for the following reasons:\n",
    "\n",
    "1. **Handling Non-Linear Relationships:**\n",
    "   - Random Forest is an ensemble learning method that is particularly effective in handling non-linear relationships in data. Credit card data often involve complex interactions between variables that linear models like logistic regression might not capture effectively.\n",
    "\n",
    "2. **Robustness to Overfitting:**\n",
    "   - While models like decision trees are prone to overfitting, Random Forest mitigates this by averaging multiple decision trees, each trained on different subsets of the data. This makes it more robust to overfitting, especially when dealing with high-dimensional data.\n",
    "\n",
    "3. **Feature Importance:**\n",
    "   - Random Forest provides useful insights into feature importance, helping us understand which features are most influential in predicting customer segments. This is valuable for interpretability and can guide business decisions.\n",
    "\n",
    "4. **Versatility and Performance:**\n",
    "   - Random Forest often performs well in a wide range of classification tasks and is less sensitive to hyperparameter tuning, making it a good initial choice for a baseline model.\n",
    "\n",
    "5. **Comparison with SVM:**\n",
    "   - Support Vector Machines (SVM) are powerful for classification problems, especially in high-dimensional spaces. However, SVM models can be computationally intensive, especially with large datasets, and require careful tuning of hyperparameters. In contrast, Random Forests are generally more scalable and easier to tune.\n",
    "\n",
    "6. **Interpretability vs. Accuracy:**\n",
    "   - Logistic regression is highly interpretable but might not provide the level of accuracy required for complex classification tasks, especially in the presence of non-linear relationships. Random Forest strikes a balance between interpretability and accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f18e1c0",
   "metadata": {},
   "source": [
    "### 6.  Feature Selection and Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9549ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "using builtin function to find optimum features\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Setup the RandomForestClassifier\n",
    "random_forest=RandomForestClassifier(n_estimators=500,random_state=1)\n",
    "random_forest.fit(X_train,y_train)\n",
    "\n",
    "# Set the columnns from the X dataset as the labels for the graph\n",
    "labels=X.columns\n",
    "\n",
    "# Select the feature importances\n",
    "feature_importances=random_forest.feature_importances_\n",
    "feature_indices=np.argsort(feature_importances)[::-1]\n",
    "\n",
    "# Select the features whose importance is greater than the mean importance\n",
    "mean_importance = feature_importances.mean()\n",
    "\n",
    "#Create a list to hold the labels of the features that qualify\n",
    "optimal_features = []\n",
    "\n",
    "for feature in range(X_train.shape[1]):\n",
    "    if feature_importances[feature_indices[feature]] > mean_importance:\n",
    "        optimal_features.append(labels[feature_indices[feature]])\n",
    "        print(\"{:2d} {:25s} {:.3f}\".format(feature+1, labels[feature_indices[feature]], \n",
    "                                       feature_importances[feature_indices[feature]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661213ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\"\"\"\n",
    "calculating mean importance to remain with the most important features only\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def calculate_feature_importances(classifier, X_data):\n",
    "    # Extract feature importances\n",
    "    feature_importances = classifier.feature_importances_\n",
    "\n",
    "    # Create a DataFrame of features and their importance scores\n",
    "    features_df = pd.DataFrame({\n",
    "        'Feature': X_data.columns,\n",
    "        'Importance': feature_importances\n",
    "    })\n",
    "\n",
    "    # Sort the DataFrame by importance\n",
    "    features_df_sorted = features_df.sort_values(by='Importance', ascending=True)\n",
    "\n",
    "    return features_df_sorted\n",
    "\n",
    "\n",
    "def extract_feature_importances(classifier, feature_data):\n",
    "    # Extract feature importances\n",
    "    feature_importances = classifier.feature_importances_\n",
    "\n",
    "    # Create a DataFrame of features and their importance scores\n",
    "    features_df = pd.DataFrame({\n",
    "        'Feature': feature_data.columns,\n",
    "        'Importance': feature_importances\n",
    "    })\n",
    "\n",
    "    # Sort the DataFrame by importance\n",
    "    features_df_sorted = features_df.sort_values(by='Importance', ascending=True)\n",
    "\n",
    "    return features_df_sorted\n",
    "\n",
    "def plot_optimal_features(features_df_sorted):\n",
    "    # Selecting only the top features (adjust the number as needed)\n",
    "    top_features = features_df_sorted.tail(5)  # Plots top 5 features\n",
    "\n",
    "    # Plotting only the top features\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.barplot(x='Importance', y='Feature', data=top_features)\n",
    "    plt.title('Top Feature Importances')\n",
    "    plt.xlabel('Importance')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.show()\n",
    "\n",
    "# Assuming 'my_classifier' is your classifier and 'my_features' is your feature dataset\n",
    "# Calculating feature importances\n",
    "feature_importance_result = extract_feature_importances(random_forest, X_train)\n",
    "\n",
    "# Plotting only the top features based on importance scores\n",
    "plot_optimal_features(feature_importance_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd2f586",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report\n",
    "\"\"\"\n",
    "Building model with selected features\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Select the data for these features\n",
    "X_new = X[optimal_features]\n",
    "\n",
    "# Split the dataset into training and testing sets (80% train, 20% test)\n",
    "X_new_train, X_new_test, y_train, y_test = train_test_split(X_new, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the classifier\n",
    "classifier = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Train the model on the training set\n",
    "classifier.fit(X_new_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_new = classifier.predict(X_new_test)\n",
    "\n",
    "# Evaluate the model using various metrics\n",
    "new_accuracy = accuracy_score(y_test, y_pred_new)\n",
    "new_precision = precision_score(y_test, y_pred_new)\n",
    "new_recall = recall_score(y_test, y_pred_new)\n",
    "new_f1 = f1_score(y_test, y_pred_new)\n",
    "new_auc_roc = roc_auc_score(y_test, y_pred_new)\n",
    "new_class_report = classification_report(y_test, y_pred_new)\n",
    "\n",
    "# Store all metrics in a dictionary\n",
    "new_model_metrics = {\n",
    "    'Accuracy': new_accuracy,\n",
    "    'Precision': new_precision,\n",
    "    'Recall': new_recall,\n",
    "    'F1 Score': new_f1,\n",
    "    'AUC-ROC': new_auc_roc\n",
    "}\n",
    "\n",
    "# Print the metrics\n",
    "print(\"New Model Metrics with Selected Features:\")\n",
    "for metric, value in new_model_metrics.items():\n",
    "    print(f\"{metric}: {value}\")\n",
    "\n",
    "# Print the classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(new_class_report)\n",
    "\n",
    "# Save the model\n",
    "model_filename = 'finalized_model.sav'\n",
    "with open(model_filename, 'wb') as file:\n",
    "    pickle.dump(classifier, file)\n",
    "\n",
    "\n",
    "cv_scores = cross_val_score(classifier, X_new_train, y_train, cv=10)\n",
    "print(\"\\nCross-validation scores: \", cv_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246be989",
   "metadata": {},
   "source": [
    "Comparing Performance of Initial Model VS New Model with selected Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b84f654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrames for each model's metrics\n",
    "df_initial = pd.DataFrame(list(initial_model_metrics.items()), columns=['Metric', 'Score'])\n",
    "df_initial['Model'] = 'Initial Model'\n",
    "\n",
    "df_new = pd.DataFrame(list(new_model_metrics.items()), columns=['Metric', 'Score'])\n",
    "df_new['Model'] = 'New Model'\n",
    "\n",
    "# Concatenate the DataFrames\n",
    "metrics_df = pd.concat([df_initial, df_new])\n",
    "\n",
    "# Plotting the model metrics\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Metric', y='Score', hue='Model', data=metrics_df)\n",
    "plt.title('Comparison of Model Metrics')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc891ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import validation_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Assuming 'X' and 'y' are already defined and 'optimal_features' is a list of feature names\n",
    "optimal_features = ['PURCHASES_FREQUENCY', 'PURCHASES_TRX', 'PURCHASES', \n",
    "                    'CASH_ADVANCE', 'CASH_ADVANCE_FREQUENCY', 'CASH_ADVANCE_TRX']\n",
    "\n",
    "X_new = X[optimal_features]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "param_range = np.arange(1, 250, 25)  # Range of n_estimators to evaluate\n",
    "\n",
    "\n",
    "\n",
    "# Validation curve for the initial model\n",
    "train_scores, test_scores = validation_curve(\n",
    "    RandomForestClassifier(max_depth=5, random_state=42),\n",
    "    X_train, y_train, param_name=\"n_estimators\", param_range=param_range,\n",
    "    cv=10, scoring=\"accuracy\", n_jobs=-1\n",
    ")\n",
    "\n",
    "# Calculate mean and standard deviation for train set scores\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "\n",
    "# Calculate mean and standard deviation for test set scores\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Validation Curve with Random Forest (Initial Model)\")\n",
    "plt.plot(param_range, train_mean, label=\"Training score\", color=\"blue\")\n",
    "plt.plot(param_range, test_mean, label=\"Cross-validation score\", color=\"red\")\n",
    "plt.fill_between(param_range, train_mean - train_std, train_mean + train_std, color=\"gray\")\n",
    "plt.fill_between(param_range, test_mean - test_std, test_mean + test_std, color=\"gainsboro\")\n",
    "plt.xlabel(\"Number of Trees\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend(loc=\"best\")\n",
    "\n",
    "X_train_OPT, X_test_OPT, y_train_OPT, y_test_OPT = train_test_split(X_new, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Validation curve for the model with optimal features\n",
    "train_scores, test_scores = validation_curve(\n",
    "    RandomForestClassifier(random_state=42),\n",
    "    X_train_OPT, y_train_OPT, param_name=\"n_estimators\", param_range=param_range,\n",
    "    cv=10, scoring=\"accuracy\", n_jobs=-1\n",
    ")\n",
    "\n",
    "# Calculate mean and standard deviation for train set scores\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "\n",
    "# Calculate mean and standard deviation for test set scores\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Validation Curve with Random Forest (Optimal Features)\")\n",
    "plt.plot(param_range, train_mean, label=\"Training score\", color=\"blue\")\n",
    "plt.plot(param_range, test_mean, label=\"Cross-validation score\", color=\"red\")\n",
    "plt.fill_between(param_range, train_mean - train_std, train_mean + train_std, color=\"gray\")\n",
    "plt.fill_between(param_range, test_mean - test_std, test_mean + test_std, color=\"gainsboro\")\n",
    "plt.xlabel(\"Number of Trees\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend(loc=\"best\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82ce358",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2e96635b",
   "metadata": {},
   "source": [
    "The graphs generated show validation curves for two Random Forest models: one using the initial set of features and another using a subset of optimal features. The validation curves plot the training and cross-validation accuracy scores as a function of the number of trees in the Random Forest (`n_estimators`).\n",
    "\n",
    "\n",
    "### Validation Curve with Random Forest (Initial Model)\n",
    "\n",
    "- **Training Score (Blue Line)**: The accuracy of the Random Forest model on the training data. It starts high and continues to increase slightly as the number of trees grows, which is typical since more trees can capture more complex patterns in the data.\n",
    "- **Cross-validation Score (Red Line)**: The accuracy of the model on the validation data. It initially increases with the number of trees, indicating that adding more trees is helping the model generalize better. However, it plateaus after a certain point, suggesting that adding more trees beyond this point does not significantly improve model performance on unseen data.\n",
    "- **Shaded Area (Gray for Training, Light Gray for Cross-validation)**: Represents the variability (one standard deviation) of the accuracy scores across the CV folds. A large shaded area indicates more variability, meaning the model's performance is more sensitive to the particular folds of the data used in cross-validation.\n",
    "\n",
    "### Validation Curve with Random Forest (Optimal Features)\n",
    "\n",
    "- **Training Score (Blue Line)**: Similar to the initial model, the training score is high, indicating good performance on the training set. However, this score starts closer to the maximum accuracy of 1.0 and remains stable, which might suggest that the model with optimal features is able to capture the patterns in the data with fewer trees.\n",
    "- **Cross-validation Score (Red Line)**: The cross-validation score is also high and stable, showing that the model with optimal features generalizes very well to unseen data. The fact that the score is consistently high across the range of `n_estimators` values suggests robustness and low overfitting.\n",
    "- **Shaded Area (Gray for Training, Light Gray for Cross-validation)**: The variability is minimal for both the training and validation scores, which is a sign of a stable model. The model's predictions are consistent across different subsets of the data.\n",
    "\n",
    "### Comparative Analysis\n",
    "\n",
    "When comparing the two graphs, it's evident that the model trained with optimal features performs better in terms of both training and validation accuracy, and it does so with less variance. The consistent high accuracy across the CV folds for the optimal features model indicates that it is a more reliable model when generalized to unseen data.\n",
    "\n",
    "In terms of the number of trees, both models show that after a certain point, increasing the number of trees does not lead to significant improvements in accuracy. This could imply that beyond this point, the benefits of adding more trees are marginal and may not justify the additional computational cost and complexity.\n",
    "\n",
    "In summary, the optimal features model not only achieves higher accuracy but also exhibits greater stability and reliability, making it the preferred model based on these validation curves."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ade7da",
   "metadata": {},
   "source": [
    "Using the metrics, it is very hard to convincingly tell which model does better, since the scores all look similar. We then resort to usng the cross validated scores for each model, which will lead us to seeing exactly which "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfe4209",
   "metadata": {},
   "source": [
    "### 7. Hyper Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b9a2bb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The benchmark model is the one with selected features, \n",
    "and therefore we now find the right parameters to tune in this cell.\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "\n",
    "\n",
    "# Select the data for these features\n",
    "X_new = X[optimal_features]\n",
    "y_new = y  \n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_new, y_new, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50,100,200],\n",
    "    'max_depth': [10,15,30],\n",
    "    'min_samples_split': range(2,10),\n",
    "    'min_samples_leaf': range(2,20)\n",
    "}\n",
    "\n",
    "# Initialize the classifier\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)\n",
    "\n",
    "# Perform the search\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best estimator\n",
    "best_rf = grid_search.best_estimator_\n",
    "\n",
    "# Save the model\n",
    "tuned_model_filename = 'tuned_random_forest_model.sav'\n",
    "with open(tuned_model_filename, 'wb') as file:\n",
    "    pickle.dump(best_rf, file)\n",
    "\n",
    "# Print out the best hyperparameters\n",
    "print(\"Best hyperparameters found: \")\n",
    "print(grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35764506",
   "metadata": {},
   "source": [
    "Justification for Selected Hyperparameters:\n",
    "\n",
    "- **n_estimators:** More trees will generally lead to better performance but also to longer training times. A grid of increasing sizes allows us to find a sweet spot.\n",
    "\n",
    "- **max_depth:** Controls the depth of each tree. Deeper trees can model more complex patterns but may lead to overfitting. Limiting tree depth can create a simpler model that may generalize better.\n",
    "\n",
    "- **min_samples_split and min_samples_leaf:** These parameters control the minimum number of samples required to split a node and to be at a leaf node, respectively, and can help prevent a tree from growing too deep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3514e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#constructing random forest model with tuned hyper parameters\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "\n",
    "# Initialize the classifier with the tuned hyperparameters\n",
    "Tuned_classifier = RandomForestClassifier(n_estimators=grid_search.best_params_['n_estimators'],\n",
    "                                    max_depth=grid_search.best_params_['max_depth'],\n",
    "                                    min_samples_split=grid_search.best_params_['min_samples_split'],\n",
    "                                    min_samples_leaf=grid_search.best_params_['min_samples_leaf'],\n",
    "                                    random_state=42)\n",
    "\n",
    "# Split the dataset into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_new, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Evaluate the model using cross-validation\n",
    "Tuned_cv_scores = cross_val_score(Tuned_classifier, X_train, y_train, cv=10)\n",
    "\n",
    "# Train the model on the entire training set\n",
    "Tuned_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = Tuned_classifier.predict(X_test)\n",
    "\n",
    "# Calculate the performance metrics\n",
    "tuned_model_metrics = {\n",
    "    'Accuracy': accuracy_score(y_test, y_pred),\n",
    "    'Precision': precision_score(y_test, y_pred),\n",
    "    'Recall': recall_score(y_test, y_pred),\n",
    "    'F1 Score': f1_score(y_test, y_pred),\n",
    "    'AUC-ROC': roc_auc_score(y_test, y_pred)\n",
    "}\n",
    "\n",
    "# Save the tuned model's cross-validation scores for later use\n",
    "tuned_model_cv_scores = list(Tuned_cv_scores)\n",
    "\n",
    "# Save the tuned model's metrics dictionary for later use\n",
    "tuned_model_metrics_dict = tuned_model_metrics\n",
    "\n",
    "# Generate the classification report\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)\n",
    "\n",
    "# Save the classification report to a text file (optional)\n",
    "with open('classification_report.txt', 'w') as f:\n",
    "    f.write(\"Classification Report:\\n\")\n",
    "    f.write(class_report)\n",
    "\n",
    "# Save the model to disk\n",
    "model_filename = 'tuned_random_forest_model.sav'\n",
    "with open(model_filename, 'wb') as file:\n",
    "    pickle.dump(Tuned_classifier, file)\n",
    "\n",
    "# Output the cross-validation scores and metrics\n",
    "print(f\"Cross-validation scores: {tuned_model_cv_scores}\")\n",
    "print(f\"Tuned Model Metrics: {tuned_model_metrics_dict}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69787b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de1ae96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert metrics to a DataFrame for visualization\n",
    "metrics_df = pd.DataFrame([new_model_metrics, tuned_model_metrics_dict], index=['Benchmark Model', 'Tuned Model']).T\n",
    "metrics_df = metrics_df.reset_index().melt(id_vars='index').rename(columns={'index': 'Metric', 'value': 'Score'})\n",
    "\n",
    "# Plotting the model metrics comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Metric', y='Score', hue='variable', data=metrics_df)\n",
    "plt.title('Comparison of Model Metrics')\n",
    "plt.xlabel('Metric')\n",
    "plt.ylabel('Score')\n",
    "plt.legend(title='Model')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6b0338",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f4e4bd9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'grid_search' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GridSearchCV\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Assuming 'grid_search' is your GridSearchCV object with the best parameters found\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m Tuned_classifier \u001b[38;5;241m=\u001b[39m RandomForestClassifier(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[43mgrid_search\u001b[49m\u001b[38;5;241m.\u001b[39mbest_params_[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m      7\u001b[0m                                     max_depth\u001b[38;5;241m=\u001b[39mgrid_search\u001b[38;5;241m.\u001b[39mbest_params_[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_depth\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m      8\u001b[0m                                     min_samples_split\u001b[38;5;241m=\u001b[39mgrid_search\u001b[38;5;241m.\u001b[39mbest_params_[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin_samples_split\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m      9\u001b[0m                                     min_samples_leaf\u001b[38;5;241m=\u001b[39mgrid_search\u001b[38;5;241m.\u001b[39mbest_params_[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin_samples_leaf\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     10\u001b[0m                                     random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Assuming 'X_train' and 'y_train' are your training data\u001b[39;00m\n\u001b[0;32m     13\u001b[0m Tuned_classifier\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'grid_search' is not defined"
     ]
    }
   ],
   "source": [
    "#saving the final benchmark model\n",
    "\n",
    "import pickle\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Assuming 'grid_search' is your GridSearchCV object with the best parameters found\n",
    "Tuned_classifier = RandomForestClassifier(n_estimators=grid_search.best_params_['n_estimators'],\n",
    "                                    max_depth=grid_search.best_params_['max_depth'],\n",
    "                                    min_samples_split=grid_search.best_params_['min_samples_split'],\n",
    "                                    min_samples_leaf=grid_search.best_params_['min_samples_leaf'],\n",
    "                                    random_state=42)\n",
    "\n",
    "# Assuming 'X_train' and 'y_train' are your training data\n",
    "Tuned_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Save the classifier as the final benchmark model\n",
    "final_benchmark_model_path = 'final_benchmark_model.pkl'\n",
    "with open(final_benchmark_model_path, 'wb') as file:\n",
    "    pickle.dump(Tuned_classifier, file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37689876",
   "metadata": {},
   "source": [
    "### 8. MODEL DEPLOYMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c44247",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install flask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a831e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf19233e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://localhost:1887\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [17/Dec/2023 09:56:19] \"GET / HTTP/1.1\" 200 -\n",
      "C:\\Users\\Rahman Mabano\\conda3\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "127.0.0.1 - - [17/Dec/2023 09:57:16] \"POST /predict HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, render_template\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Load the saved benchmark model and the scaler\n",
    "model = pickle.load(open('final_benchmark_model.pkl', 'rb'))\n",
    "\n",
    "# Define the home route\n",
    "@app.route('/')\n",
    "def home():\n",
    "    # Render the home page with form\n",
    "    return render_template('index.html')\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    try:\n",
    "        # Fetch the form inputs\n",
    "        purchase_frequency = float(request.form['PURCHASES_FREQUENCY'])\n",
    "        purchases_trx = float(request.form['PURCHASES_TRX'])\n",
    "        purchases = float(request.form['PURCHASES'])\n",
    "        cash_advance = float(request.form['CASH_ADVANCE'])\n",
    "        cash_advance_frequency = float(request.form['CASH_ADVANCE_FREQUENCY'])\n",
    "        cash_advance_trx = float(request.form['CASH_ADVANCE_TRX'])\n",
    "\n",
    "        inputs = [purchase_frequency, purchases_trx, purchases,\n",
    "                      cash_advance, cash_advance_frequency, cash_advance_trx]\n",
    "\n",
    "        input_array = np.array(inputs)\n",
    "        inputs_values = input_array.reshape(1,-1)\n",
    "            \n",
    "        result = model.predict(inputs_values)\n",
    "\n",
    "       # Generate the results that will be displayed to the user\n",
    "        if int(result)== 0:\n",
    "            predicted_class ='Premium Client'\n",
    "            color='Aquamarine'\n",
    "        else:\n",
    "            predicted_class ='Regular Client'\n",
    "            color='HoneyDew'\n",
    "\n",
    "    except ValueError as e:\n",
    "        print(f\"Caught a ValueError: {e}\")\n",
    "    \n",
    "    # Render the prediction result page\n",
    "    return render_template('predict.html', prediction=predicted_class, color_signal=color)\n",
    "\n",
    "if __name__=='__main__':\n",
    "    app.run(host='localhost', port=1887, debug=True, use_reloader=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c61b5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826c5402",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b81ab2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec9cf0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6e905d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
